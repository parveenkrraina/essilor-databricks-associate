{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff665908-cda8-4f78-b72b-676e1a645f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# End-to-End Lakehouse — Medallion Architecture \n",
    "\n",
    "**Medallion Architecture** (Bronze → Silver → Gold). \n",
    "\n",
    "### What you'll build\n",
    "- **Bronze**: raw ingestion from CSV → Delta tables, with lineage columns\n",
    "- **Silver**: cleansing, standardization, type-casting, **deduplication (Window + row_number)**, and a **quarantine** area for invalid rows\n",
    "- **Gold**: star-like business model (dimensions + fact) and daily revenue aggregates\n",
    "- **(Optional)**: incremental ingestion using **Structured Streaming** (no DLT)\n",
    "\n",
    "### How to use\n",
    "1. Upload your CSVs to DBFS (e.g., `dbfs:/FileStore/lab_data/`).  \n",
    "2. Adjust the **paths** in the next section if your filenames/locations differ.  \n",
    "3. Run the notebook from top to bottom on a Databricks cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd90e22d-b9a5-4572-b6dd-138008590694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Parameters, Imports, and Paths\n",
    "\n",
    "This cell:\n",
    "- Imports PySpark helper modules.\n",
    "- Defines a **database name** in the **legacy Hive Metastore**.\n",
    "- Builds a per-user **base path** in DBFS for Bronze/Silver/Gold layers.\n",
    "- Defines **raw CSV paths** (pointing to `FileStore/lab_data`), which you can change.\n",
    "- Ensures the **directory structure** exists using `dbutils.fs.mkdirs`.\n",
    "- Creates and selects the **Hive database** with `CREATE DATABASE IF NOT EXISTS` and `USE`.\n",
    "- Sets a sensible shuffle partitions value to optimize shuffles for medium datasets.\n",
    "\n",
    "### Functions & APIs explained\n",
    "- `from pyspark.sql import functions as F`: Imports many column functions (e.g., `col`, `trim`, `lower`, `to_date`, etc.) under the alias `F` for readability.\n",
    "- `from pyspark.sql import Window`: Used to define **window specifications** for analytics like deduplication with `row_number`.\n",
    "- `from pyspark.sql.types import *`: Imports Spark **data types** like `StructType`, `StructField`, `StringType`, etc.\n",
    "- `spark.sql(...)`: Executes SQL statements against the Spark/Hive metastore.\n",
    "- `dbutils.fs.mkdirs(path)`: Databricks utility to create folders in **DBFS**.\n",
    "- `spark.conf.set(...)`: Sets Spark conf at runtime, here to control shuffle partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331d6748-f3f7-4e62-885d-6087ed312ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ---- Database (Hive Metastore) ----\n",
    "db = \"retail_lakehouse\"  # change if you prefer a different schema/database name\n",
    "\n",
    "# ---- Build a user-specific base path so multiple users don't collide ----\n",
    "_current_user = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "user_safe = _current_user.replace('@','_').replace('.','_').replace('+','_')\n",
    "base_path = f\"dbfs:/FileStore/lakehouse/{user_safe}/retail\"\n",
    "\n",
    "# ---- Raw CSV paths (match your filenames in DBFS) ----\n",
    "# If your files are not in dbfs:/FileStore/lab_data/, upload them there or change these paths.\n",
    "raw_customers_path = \"dbfs:/FileStore/lab_data/customer_demographics.csv\"\n",
    "raw_products_path  = \"dbfs:/FileStore/lab_data/products.csv\"\n",
    "raw_sales_path     = \"dbfs:/FileStore/lab_data/sales.csv\"\n",
    "\n",
    "# ---- Derived storage layout for each medallion layer ----\n",
    "bronze_root = f\"{base_path}/bronze\"\n",
    "silver_root = f\"{base_path}/silver\"\n",
    "gold_root   = f\"{base_path}/gold\"\n",
    "quarantine_root = f\"{base_path}/quarantine\"\n",
    "\n",
    "paths = {\n",
    "    \"bronze_customers\": f\"{bronze_root}/customers\",\n",
    "    \"bronze_products\":  f\"{bronze_root}/products\",\n",
    "    \"bronze_sales\":     f\"{bronze_root}/sales\",\n",
    "    \"silver_customers\": f\"{silver_root}/customers\",\n",
    "    \"silver_products\":  f\"{silver_root}/products\",\n",
    "    \"silver_sales\":     f\"{silver_root}/sales\",\n",
    "    \"gold_dim_customer\": f\"{gold_root}/dim_customer\",\n",
    "    \"gold_dim_product\":  f\"{gold_root}/dim_product\",\n",
    "    \"gold_fact_sales\":   f\"{gold_root}/fact_sales\",\n",
    "    \"gold_sales_daily\":  f\"{gold_root}/sales_daily\",\n",
    "    \"quarantine_sales\":  f\"{quarantine_root}/sales\"\n",
    "}\n",
    "\n",
    "# ---- Create folders (idempotent) ----\n",
    "dbutils.fs.mkdirs(bronze_root)\n",
    "dbutils.fs.mkdirs(silver_root)\n",
    "dbutils.fs.mkdirs(gold_root)\n",
    "dbutils.fs.mkdirs(quarantine_root)\n",
    "\n",
    "# ---- Create and select Hive database  ----\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "spark.sql(f\"USE {db}\")\n",
    "\n",
    "# ---- Optional tuning ----\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "print(\"Environment ready in Hive Metastore, database:\", db)\n",
    "print(\"Base path:\", base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e48a431-f205-40de-9120-52e4a41eb2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Source Schemas (StructType) — why and how\n",
    "\n",
    "We **explicitly define schemas** to avoid costly inference and to catch malformed data early.\n",
    "\n",
    "### Concepts & functions\n",
    "- **`StructType([...]) / StructField(name, type, nullable)`**: Spark’s way to represent a table schema.\n",
    "- **Types used**: `StringType`, `IntegerType`, `DoubleType`, etc.  \n",
    "  We keep `*Date*` columns as **strings** in Bronze and convert to `date` in Silver for safer parsing.\n",
    "- Ingesting with an explicit schema makes CSV reading **faster and safer**.\n",
    "\n",
    "Below, the schemas are auto-generated from your CSVs so the column names match your data exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273b2e30-265d-4f8c-a8b3-c9bb1beedb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "customersSchema = StructType([\n",
    "        StructField(\"CustomerId\", StringType(), True),\n",
    "        StructField(\"CustomerName\", StringType(), True),\n",
    "        StructField(\"EmailAddress\", StringType(), True),\n",
    "        StructField(\"Region\", StringType(), True),\n",
    "        StructField(\"CustomerType\", StringType(), True)\n",
    "    ])\n",
    "productsSchema  = StructType([\n",
    "        StructField(\"Item\", StringType(), True),\n",
    "        StructField(\"Category\", StringType(), True),\n",
    "        StructField(\"ProductName\", StringType(), True),\n",
    "        StructField(\"Segment\", StringType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"LaunchDate\", StringType(), True)\n",
    "    ])\n",
    "salesSchema     = StructType([\n",
    "        StructField(\"SalesOrderNumber\", StringType(), True),\n",
    "        StructField(\"SalesOrderLineNumber\", IntegerType(), True),\n",
    "        StructField(\"OrderDate\", StringType(), True),\n",
    "        StructField(\"CustomerName\", StringType(), True),\n",
    "        StructField(\"EmailAddress\", StringType(), True),\n",
    "        StructField(\"Item\", StringType(), True),\n",
    "        StructField(\"Quantity\", DoubleType(), True),\n",
    "        StructField(\"UnitPrice\", DoubleType(), True),\n",
    "        StructField(\"TaxAmount\", DoubleType(), True),\n",
    "        StructField(\"ProductMetadata\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "print(\"Customers schema:\", [f.name + \":\" + f.dataType.simpleString() for f in customersSchema])\n",
    "print(\"Products schema:\", [f.name + \":\" + f.dataType.simpleString() for f in productsSchema])\n",
    "print(\"Sales schema:\", [f.name + \":\" + f.dataType.simpleString() for f in salesSchema])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a2c3dc-9f64-434d-b345-e8dfa8ccc1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3. BRONZE — Raw ingestion from CSV to Delta\n",
    "\n",
    "**Goal:** Land the raw CSV data **as-is** into Delta, and add **lineage columns** for traceability.\n",
    "\n",
    "### Functions used\n",
    "- `spark.read.option(\"header\", True).schema(...).csv(path)`: Reads CSV with a fixed schema.\n",
    "- `withColumn(\"ingest_ts\", current_timestamp())`: Adds an **ingestion timestamp**.\n",
    "- `withColumn(\"ingest_file\", input_file_name())`: Records the **source file** each row came from.\n",
    "- `withColumn(\"record_source\", lit(\"csv\"))`: Simple provenance marker.\n",
    "- `write.format(\"delta\").mode(\"overwrite\").save(path)`: Writes a **Delta Lake** table to storage.\n",
    "- `CREATE TABLE ... USING DELTA LOCATION '...'`: Registers the Delta folder as a **Hive Metastore table**.\n",
    "\n",
    "Why Delta? It gives you ACID transactions, time travel, and performance optimizations for analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835ecfc4-b31b-4da8-9513-3549a595b381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, current_timestamp, lit\n",
    "\n",
    "# ---- Customers to Bronze ----\n",
    "bronze_customers = (\n",
    "    spark.read.option(\"header\", True)\n",
    "        .schema(customersSchema)\n",
    "        .csv(raw_customers_path)\n",
    "        .withColumn(\"ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"ingest_file\", input_file_name())\n",
    "        .withColumn(\"record_source\", lit(\"csv\"))\n",
    ")\n",
    "bronze_customers.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(paths[\"bronze_customers\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS bronze_customers USING DELTA LOCATION '{paths['bronze_customers']}'\")\n",
    "\n",
    "# ---- Products to Bronze ----\n",
    "bronze_products = (\n",
    "    spark.read.option(\"header\", True)\n",
    "        .schema(productsSchema)\n",
    "        .csv(raw_products_path)\n",
    "        .withColumn(\"ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"ingest_file\", input_file_name())\n",
    "        .withColumn(\"record_source\", lit(\"csv\"))\n",
    ")\n",
    "bronze_products.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(paths[\"bronze_products\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS bronze_products USING DELTA LOCATION '{paths['bronze_products']}'\")\n",
    "\n",
    "# ---- Sales to Bronze ----\n",
    "bronze_sales = (\n",
    "    spark.read.option(\"header\", True)\n",
    "        .schema(salesSchema)\n",
    "        .csv(raw_sales_path)\n",
    "        .withColumn(\"ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"ingest_file\", input_file_name())\n",
    "        .withColumn(\"record_source\", lit(\"csv\"))\n",
    ")\n",
    "bronze_sales.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(paths[\"bronze_sales\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS bronze_sales USING DELTA LOCATION '{paths['bronze_sales']}'\")\n",
    "\n",
    "for t in [\"bronze_customers\",\"bronze_products\",\"bronze_sales\"]:\n",
    "    print(t, \"rows:\", spark.table(t).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e16e7b-b38b-4687-b5e2-042cefeba309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4. SILVER — Cleanse, standardize, and validate\n",
    "\n",
    "**Goal:** Produce clean, conformed data ready for analytics and joins.\n",
    "\n",
    "### Key techniques & functions\n",
    "- **Deduplication with Window**:\n",
    "  - `Window.partitionBy(keys).orderBy(ingest_ts desc, ingest_file desc)` defines the latest record per key.\n",
    "  - `F.row_number().over(window)` assigns row numbers; keeping `row_number == 1` preserves the **latest**.\n",
    "- **Column cleanup**:\n",
    "  - `trim` removes stray whitespace.\n",
    "  - `lower` standardizes case (e.g., emails).\n",
    "  - `to_date` converts safe string dates to `date` type (Silver stage is where we cast types).\n",
    "  - `cast` forces numeric types like `double` for `Quantity`, `UnitPrice`, `TaxAmount`.\n",
    "- **Validity checks**:\n",
    "  - Filter out negative prices/quantities or missing required fields.\n",
    "  - Use `exceptAll` to capture invalid rows into **quarantine** for inspection.\n",
    "\n",
    "We also keep **ingestion lineage** columns if useful for traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f97190-5693-42e5-a8aa-3b8ba77c31c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, trim, lower, to_date\n",
    "\n",
    "# Load Bronze\n",
    "b_c = spark.table(\"bronze_customers\")\n",
    "b_p = spark.table(\"bronze_products\")\n",
    "b_s = spark.table(\"bronze_sales\")\n",
    "\n",
    "def has(df, c): \n",
    "    return c in df.columns\n",
    "\n",
    "def dedup_latest(df, keys):\n",
    "    \"\"\"\n",
    "    Deduplicate by keeping the *latest* record per key set based on ingest_ts/ingest_file.\n",
    "    - keys: list of column names defining natural key (e.g., CustomerName or (SalesOrderNumber, SalesOrderLineNumber))\n",
    "    - Window.partitionBy(...).orderBy(desc) ranks rows; row_number==1 keeps the most recent.\n",
    "    \"\"\"\n",
    "    w = Window.partitionBy(*[col(k) for k in keys]).orderBy(col(\"ingest_ts\").desc(), col(\"ingest_file\").desc())\n",
    "    return df.withColumn(\"rn\", F.row_number().over(w)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "# ---- Customers (clean) ----\n",
    "cust_keys = [\"CustomerName\"] if has(b_c,\"CustomerName\") else ([\"CustomerID\"] if has(b_c,\"CustomerID\") else b_c.columns[:1])\n",
    "s_customers = dedup_latest(b_c, cust_keys)\n",
    "\n",
    "for c in [x for x in [\"CustomerName\",\"Region\",\"CustomerType\",\"EmailAddress\"] if has(s_customers,x)]:\n",
    "    s_customers = s_customers.withColumn(c, trim(col(c)))\n",
    "if has(s_customers,\"EmailAddress\"):\n",
    "    s_customers = s_customers.withColumn(\"EmailAddress\", lower(col(\"EmailAddress\")))\n",
    "\n",
    "s_customers.write.format(\"delta\").mode(\"overwrite\").save(paths[\"silver_customers\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_customers USING DELTA LOCATION '{paths['silver_customers']}'\")\n",
    "\n",
    "# ---- Products (clean) ----\n",
    "prod_keys = [\"Item\"] if has(b_p,\"Item\") else b_p.columns[:1]\n",
    "s_products = dedup_latest(b_p, prod_keys)\n",
    "\n",
    "for c in [x for x in [\"Item\",\"Category\",\"ProductName\",\"Segment\"] if has(s_products,x)]:\n",
    "    s_products = s_products.withColumn(c, trim(col(c)))\n",
    "for c in [x for x in [\"Price\"] if has(s_products,x)]:\n",
    "    s_products = s_products.withColumn(c, col(c).cast(\"double\"))\n",
    "if has(s_products,\"LaunchDate\"):\n",
    "    s_products = s_products.withColumn(\"LaunchDate\", to_date(col(\"LaunchDate\")))\n",
    "\n",
    "s_products.write.format(\"delta\").mode(\"overwrite\").save(paths[\"silver_products\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_products USING DELTA LOCATION '{paths['silver_products']}'\")\n",
    "\n",
    "# ---- Sales (clean + validate) ----\n",
    "sales_keys = [\"SalesOrderNumber\",\"SalesOrderLineNumber\"] if has(b_s,\"SalesOrderLineNumber\") and has(b_s,\"SalesOrderNumber\") else ([\"SalesOrderNumber\"] if has(b_s,\"SalesOrderNumber\") else b_s.columns[:1])\n",
    "s_sales = dedup_latest(b_s, sales_keys)\n",
    "\n",
    "for c in [x for x in [\"SalesOrderNumber\",\"CustomerName\",\"Item\",\"EmailAddress\"] if has(s_sales,x)]:\n",
    "    s_sales = s_sales.withColumn(c, trim(col(c)))\n",
    "if has(s_sales,\"EmailAddress\"):\n",
    "    s_sales = s_sales.withColumn(\"EmailAddress\", lower(col(\"EmailAddress\")))\n",
    "if has(s_sales,\"OrderDate\"):\n",
    "    s_sales = s_sales.withColumn(\"OrderDate\", to_date(col(\"OrderDate\")))\n",
    "for c in [x for x in [\"Quantity\",\"UnitPrice\",\"TaxAmount\"] if has(s_sales,x)]:\n",
    "    s_sales = s_sales.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "# Basic validity: non-null keys/dates and non-negative metrics\n",
    "valid_sales = s_sales\n",
    "if has(valid_sales,\"SalesOrderNumber\"):\n",
    "    valid_sales = valid_sales.filter(col(\"SalesOrderNumber\").isNotNull())\n",
    "if has(valid_sales,\"OrderDate\"):\n",
    "    valid_sales = valid_sales.filter(col(\"OrderDate\").isNotNull())\n",
    "if has(valid_sales,\"Quantity\"):\n",
    "    valid_sales = valid_sales.filter((col(\"Quantity\").isNotNull()) & (col(\"Quantity\") >= 0))\n",
    "if has(valid_sales,\"UnitPrice\"):\n",
    "    valid_sales = valid_sales.filter((col(\"UnitPrice\").isNotNull()) & (col(\"UnitPrice\") >= 0))\n",
    "\n",
    "invalid_sales = s_sales.exceptAll(valid_sales)\n",
    "\n",
    "valid_sales.write.format(\"delta\").mode(\"overwrite\").save(paths[\"silver_sales\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_sales USING DELTA LOCATION '{paths['silver_sales']}'\")\n",
    "\n",
    "invalid_sales.write.format(\"delta\").mode(\"overwrite\").save(paths[\"quarantine_sales\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS quarantine_sales USING DELTA LOCATION '{paths['quarantine_sales']}'\")\n",
    "\n",
    "print(\"Silver written. Quarantine count:\", spark.table(\"quarantine_sales\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d31225-da45-454f-867c-78a70665cdbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5. GOLD — Star-like model (dimensions + fact) and aggregates\n",
    "\n",
    "**Goal:** Produce analytics-ready data marts.\n",
    "\n",
    "### Steps & functions\n",
    "- Build **dimension tables** by dropping duplicates on natural keys:\n",
    "  - Customer key: use `CustomerName` if present, else `CustomerID` or first column.\n",
    "  - Product key: use `Item` if present else first column.\n",
    "- Build **fact table** by joining Silver sales with dimensions:\n",
    "  - `.join(dim_product, sales[prod_key] == dim_product.product_key, \"left\")`\n",
    "  - `.join(dim_customer, sales[cust_key] == dim_customer.customer_key, \"left\")`\n",
    "- Derive metrics:\n",
    "  - `pre_tax_amount = Quantity * UnitPrice`\n",
    "  - `total_amount = pre_tax_amount + coalesce(TaxAmount, 0.0)`\n",
    "- Derive **date parts** (`year`, `month`, `dayofmonth`, `weekofyear`) for partitioning and drill-downs.\n",
    "- **Partition** the fact by `(order_year, order_month)` to speed up time-based queries.\n",
    "\n",
    "We also create a small **daily aggregate** table for quick reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e37ebcb-fa4d-423e-ae23-6698b01138cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth, weekofyear, coalesce\n",
    "\n",
    "s_c = spark.table(\"silver_customers\")\n",
    "s_p = spark.table(\"silver_products\")\n",
    "s_s = spark.table(\"silver_sales\")\n",
    "\n",
    "# Determine dimension keys based on available columns\n",
    "cust_key = \"CustomerName\" if \"CustomerName\" in s_c.columns else (\"CustomerID\" if \"CustomerID\" in s_c.columns else s_c.columns[0])\n",
    "prod_key = \"Item\" if \"Item\" in s_p.columns else s_p.columns[0]\n",
    "\n",
    "# Dimensions\n",
    "dim_customer = s_c.dropDuplicates([cust_key]).withColumnRenamed(cust_key, \"customer_key\")\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").save(paths[\"gold_dim_customer\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS gold_dim_customer USING DELTA LOCATION '{paths['gold_dim_customer']}'\")\n",
    "\n",
    "dim_product = s_p.dropDuplicates([prod_key]).withColumnRenamed(prod_key, \"product_key\")\n",
    "dim_product.write.format(\"delta\").mode(\"overwrite\").save(paths[\"gold_dim_product\"])\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS gold_dim_product USING DELTA LOCATION '{paths['gold_dim_product']}'\")\n",
    "\n",
    "# Fact Sales\n",
    "fact = (s_s\n",
    "    .join(dim_product, s_s[prod_key] == dim_product[\"product_key\"], \"left\")\n",
    "    .join(dim_customer, s_s[cust_key] == dim_customer[\"customer_key\"], \"left\")\n",
    ")\n",
    "\n",
    "if \"Quantity\" in fact.columns and \"UnitPrice\" in fact.columns:\n",
    "    fact = fact.withColumn(\"pre_tax_amount\", F.col(\"Quantity\") * F.col(\"UnitPrice\"))\n",
    "else:\n",
    "    fact = fact.withColumn(\"pre_tax_amount\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "if \"TaxAmount\" in fact.columns:\n",
    "    fact = fact.withColumn(\"total_amount\", F.col(\"pre_tax_amount\") + coalesce(F.col(\"TaxAmount\"), F.lit(0.0)))\n",
    "else:\n",
    "    fact = fact.withColumn(\"total_amount\", F.col(\"pre_tax_amount\"))\n",
    "\n",
    "if \"OrderDate\" in fact.columns:\n",
    "    fact = (fact\n",
    "        .withColumn(\"order_year\", year(F.col(\"OrderDate\")))\n",
    "        .withColumn(\"order_month\", month(F.col(\"OrderDate\")))\n",
    "        .withColumn(\"order_day\", dayofmonth(F.col(\"OrderDate\")))\n",
    "        .withColumn(\"order_week\", weekofyear(F.col(\"OrderDate\")))\n",
    "    )\n",
    "\n",
    "# Select a curated set of columns if present\n",
    "select_cols = []\n",
    "for c in [\"SalesOrderNumber\",\"SalesOrderLineNumber\",\"OrderDate\",\"customer_key\",\"product_key\",\n",
    "          \"Quantity\",\"UnitPrice\",\"TaxAmount\",\"pre_tax_amount\",\"total_amount\",\n",
    "          \"order_year\",\"order_month\",\"order_day\",\"order_week\"]:\n",
    "    if c in fact.columns: \n",
    "        select_cols.append(c)\n",
    "\n",
    "fact = fact.select(*select_cols)\n",
    "\n",
    "partition_cols = [c for c in [\"order_year\",\"order_month\"] if c in fact.columns]\n",
    "(fact.write.format(\"delta\").mode(\"overwrite\").partitionBy(partition_cols).save(paths[\"gold_fact_sales\"]))\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS gold_fact_sales USING DELTA LOCATION '{paths['gold_fact_sales']}'\")\n",
    "\n",
    "# Daily aggregate\n",
    "if \"OrderDate\" in fact.columns:\n",
    "    gold_sales_daily = (fact.groupBy(\"OrderDate\")\n",
    "        .agg(F.sum(\"Quantity\").alias(\"total_qty\") if \"Quantity\" in fact.columns else F.lit(None).alias(\"total_qty\"),\n",
    "             F.sum(\"total_amount\").alias(\"total_revenue\") if \"total_amount\" in fact.columns else F.lit(None).alias(\"total_revenue\"))\n",
    "    )\n",
    "    (gold_sales_daily.write.format(\"delta\").mode(\"overwrite\").save(paths[\"gold_sales_daily\"]))\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS gold_sales_daily USING DELTA LOCATION '{paths['gold_sales_daily']}'\")\n",
    "\n",
    "print(\"Gold model created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43cf685c-fa03-4351-aa14-1b8a08d05880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 6. (Optional) Performance & Maintenance — OPTIMIZE / Z-ORDER / VACUUM\n",
    "\n",
    "**Delta Lake** includes helpful commands:\n",
    "- `OPTIMIZE table ZORDER BY (...)` clusters data files by the given columns to speed up range queries.\n",
    "- `VACUUM table RETAIN <HOURS> HOURS` cleans old files. Be mindful of your retention policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4819357a-d414-4739-81b7-71a1bfdd6c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# These are optional; uncomment if you have sufficient privileges and the tables exist.\n",
    "# spark.sql(\"OPTIMIZE gold_fact_sales ZORDER BY (OrderDate)\")\n",
    "# spark.sql(\"OPTIMIZE gold_sales_daily ZORDER BY (OrderDate)\")\n",
    "# spark.sql(\"VACUUM gold_fact_sales RETAIN 168 HOURS\")   # keep 7 days of history\n",
    "# spark.sql(\"VACUUM gold_sales_daily RETAIN 168 HOURS\")\n",
    "print(\"Optional maintenance commands are commented out by default.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f146de9-fc3b-40b3-8dd8-47097151b9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 7. Validation and Sample Queries\n",
    "\n",
    "This section verifies row counts and shows sample results to confirm the pipeline produced usable outputs.\n",
    "\n",
    "- `spark.table(name).count()` — quick sanity checks.\n",
    "- `display(...)` — Databricks utility to render tables in the UI.\n",
    "- Example queries: **Top orders by revenue**, recent days’ aggregates, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db50b81a-6467-4026-ae8f-c51c6a0aa58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tables = [\"bronze_customers\",\"bronze_products\",\"bronze_sales\",\n",
    "          \"silver_customers\",\"silver_products\",\"silver_sales\",\"quarantine_sales\",\n",
    "          \"gold_dim_customer\",\"gold_dim_product\",\"gold_fact_sales\"]\n",
    "\n",
    "# gold_sales_daily may not exist if OrderDate was absent; handle safely\n",
    "try:\n",
    "    spark.table(\"gold_sales_daily\")\n",
    "    tables.append(\"gold_sales_daily\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Row counts:\")\n",
    "for t in tables:\n",
    "    try:\n",
    "        print(t, spark.table(t).count())\n",
    "    except Exception as e:\n",
    "        print(t, \"ERROR:\", e)\n",
    "\n",
    "print(\"\\nSample: Top 10 sales orders by total_amount (if column exists)\")\n",
    "try:\n",
    "    display(spark.table(\"gold_fact_sales\").orderBy(F.desc(\"total_amount\")).limit(10))\n",
    "except Exception as e:\n",
    "    print(\"Display failed:\", e)\n",
    "\n",
    "print(\"\\nSample: Last 20 days in gold_sales_daily (if table exists)\")\n",
    "try:\n",
    "    display(spark.table(\"gold_sales_daily\").orderBy(F.col(\"OrderDate\").desc()).limit(20))\n",
    "except Exception as e:\n",
    "    print(\"Display failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f852cf1-2f70-4cff-b627-4cda23ee2cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 8. (Optional) Incremental Ingestion with **Structured Streaming**\n",
    "You can make Bronze ingestion incremental by watching a **directory** for new CSV files:\n",
    "\n",
    "### Concepts & functions\n",
    "- `spark.readStream.format(\"csv\").schema(...).load(dir)`: Read new files that land in `dir`.\n",
    "- `withColumn(\"ingest_ts\", current_timestamp())`, etc.: Same lineage columns as batch.\n",
    "- `writeStream.format(\"delta\").option(\"checkpointLocation\", path).outputMode(\"append\").start(dest)`:\n",
    "  - **`checkpointLocation`** stores offsets and state so the stream can resume safely.\n",
    "  - **`availableNow=True`** processes all current files and then stops (good for micro-batch backfills).\n",
    "\n",
    "> Set `raw_sales_path` to a **directory** (e.g., `dbfs:/FileStore/lab_data/sales/`) and drop new CSVs there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4486d7a9-55ef-44db-9280-c4e5ee09ef18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example streaming job (disabled by default).\n",
    "# Ensure raw_sales_path points to a DIRECTORY with incoming files before enabling.\n",
    "\n",
    "# from pyspark.sql.functions import input_file_name, current_timestamp, lit\n",
    "# checkpoint_dir = f\"{base_path}/_checkpoints/bronze_sales\"\n",
    "\n",
    "# stream_df = (spark.readStream\n",
    "#     .format(\"csv\")\n",
    "#     .option(\"header\", True)\n",
    "#     .schema(salesSchema)              # enforce schema\n",
    "#     .load(raw_sales_path)             # DIRECTORY with new files arriving\n",
    "#     .withColumn(\"ingest_ts\", current_timestamp())\n",
    "#     .withColumn(\"ingest_file\", input_file_name())\n",
    "#     .withColumn(\"record_source\", lit(\"csv_stream\"))\n",
    "# )\n",
    "\n",
    "# query = (stream_df.writeStream\n",
    "#     .format(\"delta\")\n",
    "#     .option(\"checkpointLocation\", checkpoint_dir)\n",
    "#     .outputMode(\"append\")\n",
    "#     .trigger(availableNow=True)       # process existing files then stop\n",
    "#     .start(paths[\"bronze_sales\"])\n",
    "# )\n",
    "# query.awaitTermination()\n",
    "# print(\"Streaming ingestion completed (availableNow).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89d7c417-f20a-43ab-9449-c783094fe881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 9. Troubleshooting & FAQs\n",
    "\n",
    "- **File not found**: Make sure your CSVs are in `dbfs:/FileStore/lab_data/` or update the `raw_*_path` variables.\n",
    "- **Schema mismatch**: Adjust the `customersSchema/productsSchema/salesSchema` in the Schema cell.\n",
    "- **Delta errors**: Ensure your cluster runtime supports Delta (all current DBR versions do).\n",
    "- **Performance**: Tune `spark.sql.shuffle.partitions` and consider enabling the optional **OPTIMIZE**/`ZORDER` steps.\n",
    "- **No Unity Catalog**: This notebook uses `CREATE DATABASE`/`USE` only; **no `USE CATALOG`** or catalog prefixes are present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97984729-b340-497f-bd94-f01632eddb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Technique           | Purpose                           | Frequency            | Pros                                      | Cons                                |\n",
    "|---------------------|-----------------------------------|----------------------|-------------------------------------------|-------------------------------------|\n",
    "| **OPTIMIZE**        | Compact small files into larger   | Weekly/Daily         | Speeds queries, reduces file overhead     | Compute cost for rewriting data     |\n",
    "| **Z-ORDER**         | Cluster data for better skipping  | With OPTIMIZE        | Faster queries on multi-column filters    | Rewrite cost; choose ≤3 columns     |\n",
    "| **Auto Optimize**   | Optimize during writes            | Always on (if fits)  | No manual compaction, avoids small files  | Slightly slower writes              |\n",
    "| **VACUUM**          | Delete old unreferenced files     | Weekly+              | Saves storage, keeps storage clean        | Reduces time travel capability      |\n",
    "| **Partitioning**    | Reduce scanned data by directory  | Table design time    | Large query speedup for targeted filters  | Over-partitioning → small files     |\n",
    "| **Predicate Pushdown** | Push filters to storage layer | Always               | Reads less data, improves performance     | Limited by available file statistics|\n",
    "| **Caching**         | Keep hot data in memory           | Job runtime          | Very fast re-use of data                  | Consumes cluster memory              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70799a5a-974b-41e7-960c-444a63c17340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lakehouse End To End",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}