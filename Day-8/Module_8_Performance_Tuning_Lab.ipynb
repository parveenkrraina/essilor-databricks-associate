{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dfc1a03-3961-4505-b294-9e138ef1079f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Module 8 Lab: Performance Tuning & Cost Optimization on Databricks\n",
    "**Language:** PySpark (with SQL references)  \n",
    "**Goal:** Measure performance improvements from Spark- and Delta-level optimizations.\n",
    "\n",
    "> Run top-to-bottom on a Databricks cluster. Keep cluster size constant within a given A/B test for fair comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1d9b98-3d26-439b-b784-a10d2b829e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ðŸ”­ Big Picture: What You'll Do\n",
    "1. **Ingest** CSV data â†’ Delta tables with an intentionally suboptimal layout.\n",
    "2. Record **baseline** query plans & runtimes.\n",
    "3. Apply **Spark optimizations** (AQE, predicate pushdown, projection pruning, broadcast joins, caching).\n",
    "4. Apply **Delta optimizations** (OPTIMIZE, Z-ORDER, Auto Optimize).\n",
    "5. **Re-benchmark** and compare end-to-end speedups & cost considerations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63d7111-da98-4447-a11d-077ebdd494a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Notebook 0 â€” Utilities\n",
    "\n",
    "Below we define two reusable helpers:\n",
    "\n",
    "- `bench(name)`: a **decorator** that times any function you run. It prints how long the wrapped code took and returns both the result and the timing value.  \n",
    "  - Uses Python's `time.time()` to mark start/end and `functools.wraps` so your function's name/docstring stay intact.\n",
    "\n",
    "- `print_plan(df)`: prints Spark's **query execution plan**. We try the JVM-native plan (`_jdf.queryExecution().toString()`), then fall back to `df.explain(\"formatted\")`. We also try `df.explain(\"cost\")` (if supported) to see cost-based estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a66a04-48b9-4b33-98de-3dda685dab36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def bench(name):\n",
    "    \"\"\"Decorator to time a function and print elapsed seconds.\"\"\"\n",
    "    def decorator(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            print(f\"\\n=== {name} ===\")\n",
    "            t0 = time.time()\n",
    "            result = fn(*args, **kwargs)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"{name} took: {dt:.3f}s\")\n",
    "            return result, dt\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def print_plan(df):\n",
    "    \"\"\"Print Spark query plans for inspection.\"\"\"\n",
    "    try:\n",
    "        print(\"\\n-- EXPLAIN (formatted via JVM) --\")\n",
    "        print(df._jdf.queryExecution().toString())  # engine-native plan text\n",
    "    except Exception as e:\n",
    "        print(\"Formatted plan via _jdf not available:\", e)\n",
    "        df.explain(\"formatted\")\n",
    "    print(\"\\n-- EXPLAIN cost --\")\n",
    "    try:\n",
    "        df.explain(\"cost\")\n",
    "    except Exception as e:\n",
    "        print(\"Cost-based EXPLAIN not supported on this runtime:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41499a0a-2832-4d07-8b7c-3e76149fff0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 1 â€” Environment Setup\n",
    "\n",
    "We create a working **database** and choose a **DBFS path** to store Delta tables.\n",
    "\n",
    "- `spark.sql(...)`: runs SQL statements from Python.\n",
    "- Keep cluster/runtime consistent between baseline and optimized tests for fair comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66179295-c6c0-41cc-bee4-e249ff0782d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Choose working catalog/database and DBFS base path\n",
    "catalog = \"hive_metastore\"            # change if using Unity Catalog ('main', etc.)\n",
    "db = \"perf_tuning_lab\"\n",
    "base_path = \"/FileStore/tmp/perf_tuning_lab\"    # DBFS path for this lab\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "spark.sql(f\"USE {db}\")\n",
    "\n",
    "print(\"Using catalog:\", catalog, \"| database:\", db, \"| base_path:\", base_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862a814b-cb44-40d5-9bc2-39a93a93d786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ðŸ“¥ Upload CSVs to DBFS\n",
    "\n",
    "Upload these files to the following DBFS directory **before** running Step 1.2:\n",
    "- `dbfs:/FileStore/tmp/perf_tuning_lab/input/sales.csv`\n",
    "- `dbfs:/FileStore/tmp/perf_tuning_lab/input/product_catalog.csv`\n",
    "- `dbfs:/FileStore/tmp/perf_tuning_lab/input/customer_details.csv`\n",
    "\n",
    "You can do this via **Catalog â†’ Browse DBFS** in the Databricks UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18440c7-bdd9-46b6-a3d3-c36326cdd5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1.2 Read CSVs & Create a Scaled Dataset \n",
    "\n",
    "- `spark.read.csv(...).option(\"header\",\"true\")`: first row contains column names.\n",
    "- `.option(\"inferSchema\",\"true\")`: Spark infers column types (convenient; explicit schema is faster for production).\n",
    "- We **scale up** rows by repeatedly `union`-ing the same DataFrame and modifying a key column so it's unique. This helps stress the system to make optimizations visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387297fd-ffa6-41b2-b8c7-2c4e27be013e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SCALE_SALES = 10  # increase to stress the system\n",
    "\n",
    "raw_sales = (spark.read\n",
    "  .option(\"header\",\"true\").option(\"inferSchema\",\"true\")\n",
    "  .csv(f\"{base_path}/input/sales.csv\"))\n",
    "\n",
    "raw_products = (spark.read\n",
    "  .option(\"header\",\"true\").option(\"inferSchema\",\"true\")\n",
    "  .csv(f\"{base_path}/input/product_catalog.csv\"))\n",
    "\n",
    "raw_customers = (spark.read\n",
    "  .option(\"header\",\"true\").option(\"inferSchema\",\"true\")\n",
    "  .csv(f\"{base_path}/input/customer_details.csv\"))\n",
    "\n",
    "# Synthetic scale-up with union; we modify SalesOrderLineNumber to keep rows unique\n",
    "scaled_sales = raw_sales\n",
    "for i in range(SCALE_SALES-1):\n",
    "    scaled_sales = scaled_sales.union(\n",
    "        raw_sales.withColumn(\n",
    "            \"SalesOrderLineNumber\",\n",
    "            F.col(\"SalesOrderLineNumber\") + F.lit(100000*(i+1))\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Row counts â†’\",\n",
    "      \"sales:\", scaled_sales.count(),\n",
    "      \"| products:\", raw_products.count(),\n",
    "      \"| customers:\", raw_customers.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7504dd4a-a3c1-47e7-8767-e7d04a26900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2 â€” Ingest to Delta\n",
    "\n",
    "We normalize types and compute `SalesAmount`. Then we write to **Delta** without partitioning and with **high `repartition`** to **create many small files** intentionally (to demonstrate how `OPTIMIZE` and Z-ORDER help later).\n",
    "\n",
    "**Key functions:**\n",
    "- `withColumn`, `F.to_date`, `cast`: type cleanup.\n",
    "- `repartition(200)`: increases output file count (we will compact later).\n",
    "- `WRITE format('delta')`: saves as Delta files.\n",
    "- `CREATE TABLE ... USING delta LOCATION ...`: registers an external Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e668ea-acbc-404f-b8db-805c7f3996d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sales_df = (scaled_sales\n",
    "  .withColumn(\"OrderDate\", F.to_date(\"OrderDate\"))\n",
    "  .withColumn(\"Quantity\", F.col(\"Quantity\").cast(\"int\"))\n",
    "  .withColumn(\"UnitPrice\", F.col(\"UnitPrice\").cast(\"double\"))\n",
    "  .withColumn(\"TaxAmount\", F.col(\"TaxAmount\").cast(\"double\"))\n",
    "  .withColumn(\"SalesAmount\", F.col(\"Quantity\") * F.col(\"UnitPrice\") + F.col(\"TaxAmount\"))\n",
    ")\n",
    "\n",
    "product_df = raw_products\n",
    "customer_df = raw_customers\n",
    "\n",
    "(sales_df\n",
    "  .repartition(200)  # intentionally many files\n",
    "  .write.mode(\"overwrite\").format(\"delta\")\n",
    "  .save(f\"{base_path}/tables/sales_delta\"))\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS sales_delta USING delta LOCATION '{base_path}/tables/sales_delta'\")\n",
    "\n",
    "(product_df.write.mode(\"overwrite\").format(\"delta\")\n",
    "  .save(f\"{base_path}/tables/product_delta\"))\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS product_delta USING delta LOCATION '{base_path}/tables/product_delta'\")\n",
    "\n",
    "(customer_df.write.mode(\"overwrite\").format(\"delta\")\n",
    "  .save(f\"{base_path}/tables/customer_delta\"))\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS customer_delta USING delta LOCATION '{base_path}/tables/customer_delta'\")\n",
    "\n",
    "print(\"Delta tables ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ff7fe9-5053-4fb7-a405-700d2c2e4637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### âœ… Checkpoint A â€” Validate Layout & Files\n",
    "- `DESCRIBE DETAIL` shows Delta metadata (`numFiles`, `sizeInBytes`, etc.).  \n",
    "- `dbutils.fs.ls(path)` lists the actual files. Expect **many small files** now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e9787b-d892-4ee7-82c1-08d3afa416d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"DESCRIBE DETAIL sales_delta\").show(truncate=False)\n",
    "display(dbutils.fs.ls(f\"{base_path}/tables/sales_delta\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3681441-721f-4bb5-a688-5968997f6950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 3 â€” Baseline Queries\n",
    "\n",
    "We **disable** Spark-side optimizations to get a clean baseline:\n",
    "- `spark.sql.adaptive.enabled=false`: turn off Adaptive Query Execution (AQE).\n",
    "- `spark.sql.autoBroadcastJoinThreshold=-1`: prevent auto-broadcast joins.\n",
    "\n",
    "We then run three representative analytics:\n",
    "1. **Q1**: Filter + aggregate (tests predicate pushdown & projection pruning potential)\n",
    "2. **Q2**: Join with product dimension (tests broadcast vs shuffle)\n",
    "3. **Q3**: Star join and aggregations (closer to real-world ETL/reporting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b7d019-8415-4a41-bc4a-68e77b325145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "print(\"Baseline mode: AQE OFF, auto-broadcast OFF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc162ecc-33f4-48b1-b3e6-59a4945dda0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Q1 â€” Baseline\n",
    "\n",
    "- `spark.table(\"sales_delta\")`: read a Delta table as a DataFrame.  \n",
    "- `filter(...)`: time-range filter (this should push down to storage when possible).  \n",
    "- `groupBy().agg(F.sum(...))`: compute total sales.  \n",
    "- `orderBy(desc)`: sort to get top items.  \n",
    "- We call `print_plan()` to verify plan (look for scans, filters, and shuffles).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb41c63e-2cd4-46ab-8dca-3c15a111014b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@bench(\"Q1 baseline\")\n",
    "def run_q1():\n",
    "    from pyspark.sql import functions as F\n",
    "    df = (spark.table(\"sales_delta\")\n",
    "            .filter(\"OrderDate >= '2018-01-01' AND OrderDate < '2019-01-01'\")\n",
    "            .groupBy(\"Item\")\n",
    "            .agg(F.sum(\"SalesAmount\").alias(\"TotalSales\"))\n",
    "            .orderBy(F.desc(\"TotalSales\")))\n",
    "    print_plan(df)\n",
    "    return df.collect()\n",
    "\n",
    "q1_result, q1_time_base = run_q1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e11263-3f1f-4035-a4f9-bd5fd07f1a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Q2 â€” Baseline Join\n",
    "\n",
    "- `join(prod, \"Item\")` without broadcast causes a **shuffle** join by default.  \n",
    "- We aggregate by `Category` to see top categories.  \n",
    "- Again, inspect the plan to confirm join type and number of shuffles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1735be8f-ee9c-4a80-a9a3-96cb6cab5f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@bench(\"Q2 baseline join\")\n",
    "def run_q2():\n",
    "    from pyspark.sql import functions as F\n",
    "    sales = spark.table(\"sales_delta\")\n",
    "    prod  = spark.table(\"product_delta\")\n",
    "    df = (sales.join(prod, \"Item\")\n",
    "               .filter(\"OrderDate >= '2018-01-01' AND OrderDate < '2019-01-01'\")\n",
    "               .groupBy(\"Category\")\n",
    "               .agg(F.sum(\"SalesAmount\").alias(\"TotalSales\"))\n",
    "               .orderBy(F.desc(\"TotalSales\")))\n",
    "    print_plan(df)\n",
    "    return df.collect()\n",
    "\n",
    "q2_result, q2_time_base = run_q2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78620cc9-a488-4592-9647-d2dd9ad089db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Q3 â€” Baseline Star Join\n",
    "\n",
    "- Fact `sales` joins Product and Customer dims.  \n",
    "- Filters to `Country = 'United States'` and a given year.  \n",
    "- Aggregates to state/category level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7675f5-5e9d-4973-a20d-d15c5dce9684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@bench(\"Q3 baseline star\")\n",
    "def run_q3():\n",
    "    sales = spark.table(\"sales_delta\")\n",
    "    prod  = spark.table(\"product_delta\")\n",
    "    cust  = spark.table(\"customer_delta\")\n",
    "    df = (\n",
    "        sales.join(prod, \"Item\")\n",
    "        .join(cust, \"CustomerID\")\n",
    "        .filter(\n",
    "            (F.col(\"Country\") == \"United States\") &\n",
    "            (F.col(\"OrderDate\") >= \"2019-01-01\") &\n",
    "            (F.col(\"OrderDate\") < \"2020-01-01\")\n",
    "        )\n",
    "        .groupBy(\"State\", \"Category\")\n",
    "        .agg(\n",
    "            F.sum(\"SalesAmount\").alias(\"TotalSales\"),\n",
    "            F.countDistinct(\"SalesOrderNumber\").alias(\"Orders\")\n",
    "        )\n",
    "        .orderBy(F.desc(\"TotalSales\"))\n",
    "    )\n",
    "    print_plan(df)\n",
    "    return df.collect()\n",
    "\n",
    "q3_result, q3_time_base = run_q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8d850f-c8c2-4bba-b227-c725d3104635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 4 â€” Spark Optimizations\n",
    "\n",
    "We now enable Spark features that often provide **2â€“5x** gains:\n",
    "\n",
    "- **AQE** (`spark.sql.adaptive.enabled=true`): dynamically optimizes plans at runtime (e.g., coalesces small partitions, handles skewed joins).  \n",
    "- **Predicate Pushdown & Projection Pruning**: filter early and select only needed columns to reduce I/O and shuffle widths.  \n",
    "- **Broadcast Joins**: replicate small dimension to all executors to avoid a large shuffle.  \n",
    "- **Caching**: store reused intermediate results to accelerate iterative queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ced17b1-d898-4053-8eed-3e592de52aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "print(\"AQE + skew join handling + coalesce enabled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9030b787-9393-4e2c-8df3-0a012ab02836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Q1 Optimized â€” Predicate Pushdown & Projection Pruning \n",
    "\n",
    "- `select(*cols_needed)`: **projection pruning** reduces scanned columns and shuffle width.  \n",
    "- Early `filter(...)`: encourages **predicate pushdown** to the data source.  \n",
    "- Same aggregation logic as baseline; plan should be slimmer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cb0738-7b80-4e43-8ebb-fd884409e64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@bench(\"Q1 optimized (pushdown + projection)\")\n",
    "def run_q1_opt():\n",
    "    from pyspark.sql import functions as F\n",
    "    cols_needed = [\"Item\",\"SalesAmount\",\"OrderDate\"]\n",
    "    sales_pruned = (spark.table(\"sales_delta\")\n",
    "                      .select(*cols_needed)\n",
    "                      .filter(\"OrderDate >= '2018-01-01' AND OrderDate < '2019-01-01'\"))\n",
    "    df = (sales_pruned.groupBy(\"Item\")\n",
    "           .agg(F.sum(\"SalesAmount\").alias(\"TotalSales\"))\n",
    "           .orderBy(F.desc(\"TotalSales\")))\n",
    "    print_plan(df)\n",
    "    return df.collect()\n",
    "\n",
    "q1_opt_result, q1_time_opt = run_q1_opt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34bb994e-f558-428b-bb03-913e9075f247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Q2 Optimized â€” Broadcast Join\n",
    "\n",
    "- Set `spark.sql.autoBroadcastJoinThreshold` ~ 10MB, or explicitly use `broadcast(df)`.  \n",
    "- Broadcast avoids shuffling the small dimension side, eliminating a big cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c31e2a-a792-4344-98f7-168c9f88f53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10*1024*1024)  # ~10MB\n",
    "\n",
    "@bench(\"Q2 optimized (broadcast)\")\n",
    "def run_q2_opt():\n",
    "    from pyspark.sql import functions as F\n",
    "    sales = (spark.table(\"sales_delta\")\n",
    "               .select(\"Item\",\"SalesAmount\",\"OrderDate\")\n",
    "               .filter(\"OrderDate >= '2018-01-01' AND OrderDate < '2019-01-01'\"))\n",
    "    prod  = broadcast(spark.table(\"product_delta\").select(\"Item\",\"Category\"))\n",
    "    df = (sales.join(prod, \"Item\")\n",
    "               .groupBy(\"Category\")\n",
    "               .agg(F.sum(\"SalesAmount\").alias(\"TotalSales\"))\n",
    "               .orderBy(F.desc(\"TotalSales\")))\n",
    "    print_plan(df)\n",
    "    return df.collect()\n",
    "\n",
    "q2_opt_result, q2_time_opt = run_q2_opt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8825ee35-4456-4280-be91-6c40baf19403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Q3 Optimized â€” Caching + AQE + Broadcast\n",
    "\n",
    "- `cache()` marks the DataFrame to be kept in memory (or memory+disk).  \n",
    "- We **materialize** the cache with a `count()` so future actions are fast.  \n",
    "- Combine with broadcast and AQE for best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181a45c9-7b84-4d34-91f8-227e5d58d5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cache a hot subset for reuse\n",
    "sales_2019 = (\n",
    "    spark.table(\"sales_delta\")\n",
    "    .select(\"Item\", \"CustomerID\", \"OrderDate\", \"SalesAmount\")\n",
    "    .filter(\"OrderDate >= '2019-01-01' AND OrderDate < '2020-01-01'\")\n",
    "    .cache()\n",
    ")\n",
    "sales_2019.count()  # materialize cache\n",
    "\n",
    "@bench(\"Q3 optimized (cache + AQE + broadcast)\")\n",
    "def run_q3_opt():\n",
    "    prod = broadcast(\n",
    "        spark.table(\"product_delta\").select(\"Item\", \"Category\")\n",
    "    )\n",
    "    cust = (\n",
    "        spark.table(\"customer_delta\")\n",
    "        .select(\"CustomerID\", \"CustomerName\", \"EmailAddress\", \"Country\", \"State\")\n",
    "    )\n",
    "    df = (\n",
    "        sales_2019.join(prod, \"Item\")\n",
    "        .join(cust, \"CustomerID\")\n",
    "        .filter(\"Country = 'United States'\")\n",
    "        .groupBy(\"State\", \"Category\")\n",
    "        .agg(\n",
    "            F.sum(\"SalesAmount\").alias(\"TotalSales\"),\n",
    "            F.countDistinct(\"CustomerName\").alias(\"DistinctCustomers\")\n",
    "        )\n",
    "        .orderBy(F.desc(\"TotalSales\"))\n",
    "    )\n",
    "    print_plan(df)\n",
    "    return df.collect()\n",
    "\n",
    "q3_opt_result, q3_time_opt = run_q3_opt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f3e313-40d9-44dc-91b4-5f7dc871b006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 5 â€” Delta Lake Optimizations\n",
    "\n",
    "- **`OPTIMIZE`** compacts many small files into fewer, larger files (targeting ~256MBâ€“1GB). This reduces metadata ops and improves scan throughput.  \n",
    "- **`ZORDER BY (col1, col2, ...)`** physically clusters data so selective queries can **skip** more data. Choose columns that are frequently filtered/joined **together**.  \n",
    "- **Auto Optimize** (`optimizeWrite` + `autoCompact`) keeps future writes healthy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5741af8d-d784-4144-b178-2c2979c4bd15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Inspect before/after compaction\n",
    "spark.sql(\"DESCRIBE DETAIL sales_delta\").show(truncate=False)\n",
    "spark.sql(\"OPTIMIZE sales_delta\")\n",
    "spark.sql(\"DESCRIBE DETAIL sales_delta\").show(truncate=False)\n",
    "\n",
    "# Z-ORDER on common filter/join keys\n",
    "spark.sql(\"\"\"\n",
    "  OPTIMIZE sales_delta\n",
    "  ZORDER BY (OrderDate, Item)\n",
    "\"\"\")\n",
    "\n",
    "# Enable Auto Optimize going forward\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE sales_delta SET TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "    'delta.autoOptimize.autoCompact'   = 'true'\n",
    "  )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb50eb5a-2eff-44f2-bd6e-fc3ff4a7328e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### âœ… Checkpoint D â€” Storage-only Gains\n",
    "\n",
    "We now **turn off Spark hints** again to isolate the storage layout impact from Delta operations. Expect speedups where filters hit Z-ordered columns, and fewer files reduce overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32cc2e82-8925-4860-860e-57640002ed43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.catalog.clearCache()\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "_, q1_time_after_delta = run_q1()\n",
    "_, q2_time_after_delta = run_q2()\n",
    "_, q3_time_after_delta = run_q3()\n",
    "\n",
    "print(f\"Delta-only speedup Q1: {q1_time_base/q1_time_after_delta:.2f}x\")\n",
    "print(f\"Delta-only speedup Q2: {q2_time_base/q2_time_after_delta:.2f}x\")\n",
    "print(f\"Delta-only speedup Q3: {q3_time_base/q3_time_after_delta:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fa2fee-8909-4176-8c90-3f86c54e4086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 6 â€” Combined Optimizations\n",
    "\n",
    "Finally, we **stack** Spark and Delta optimizations for the best overall performance per dollar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7933c924-35ef-4998-a1b9-9e1dff5e9871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10*1024*1024)\n",
    "\n",
    "_, q1_time_final = run_q1_opt()\n",
    "_, q2_time_final = run_q2_opt()\n",
    "_, q3_time_final = run_q3_opt()\n",
    "\n",
    "print(\"--- OVERALL SPEEDUPS (vs. original baseline) ---\")\n",
    "print(f\"Q1 overall: {q1_time_base/q1_time_final:.2f}x\")\n",
    "print(f\"Q2 overall: {q2_time_base/q2_time_final:.2f}x\")\n",
    "print(f\"Q3 overall: {q3_time_base/q3_time_final:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25094fc9-66d3-438e-9e62-1c4b9ee419e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 7 â€” Cost Awareness & Cluster Tuning\n",
    "\n",
    "- **Fixed vs Autoscaling**: Autoscaling reduces idle cost but may add ramp-up time.  \n",
    "- **Spot Instances**: Much cheaper but can be preemptedâ€”great for non-SLA batch ETL.  \n",
    "- **DBU Estimation**: Approximate cost = (DBU/hr Ã— runtime hrs Ã— DBU price) + infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50247d23-5c58-40d7-80e2-4803a3990583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Cleanup (Optional)\n",
    "Uncomment if you want to reclaim space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2d52c9-bf8b-4a33-98b0-7789f1a0fc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# spark.sql(\"VACUUM sales_delta RETAIN 168 HOURS\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS sales_delta\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS product_delta\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS customer_delta\")\n",
    "# dbutils.fs.rm(base_path, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a580d3-5785-4ff4-b46c-c6a8b95c45d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ðŸ“Ž Quick Reference â€” Functions & Why\n",
    "\n",
    "- **`bench(name)`**: Decorator timing harness to compare baseline vs optimized.  \n",
    "- **`print_plan(df)` / `df.explain(...)`**: Verify filter pushdown, join types, shuffles.  \n",
    "- **Read CSV**: `.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(path)`  \n",
    "- **Transform**: `withColumn`, `F.to_date`, `cast`, arithmetic on `Column`s  \n",
    "- **Shuffle width**: `repartition(n)` controls output files & parallelism  \n",
    "- **Delta**: `OPTIMIZE`, `ZORDER BY`, table `TBLPROPERTIES` for auto optimize  \n",
    "- **Spark configs**: `spark.conf.set(...)` for AQE & joins  \n",
    "- **Broadcast**: `from pyspark.sql.functions import broadcast; broadcast(df)`  \n",
    "- **Cache**: `df.cache(); df.count()` materializes reuse\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Module_8_Performance_Tuning_Lab",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
